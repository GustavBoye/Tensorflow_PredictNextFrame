import os
import random
import matplotlib.pyplot as plt
import tensorflow as tf
from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow.python.client import device_lib


def load_images(folder_path):
    images = []
    image_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])
    for image_file in image_files:
        image_path = os.path.join(folder_path, image_file)
        image = Image.open(image_path)
        image = image.resize((64, 64))  # Resize for faster training
        image = np.array(image) / 255.0  # Normalize pixel values
        images.append(image)
    return np.array(images)



def create_batch(images, batch_size):
    batch_input_frames = []
    batch_output_frames = []


    for i in range(batch_size):
        start_index = random.randint(0, len(images)-20)
        input_frames = images[start_index:start_index + 4]  # Results in (4, 64, 64, 3)

        stacked_input_frames = np.concatenate(input_frames,
                                              axis=-1)  # Stack input frames along the last axis, results in (64, 64, 12)

        output_frame = images[start_index + 4]

        batch_input_frames.append(stacked_input_frames)
        batch_output_frames.append(output_frame)

    return np.array(batch_input_frames), np.array(batch_output_frames)


def build_generator():
    inputs = tf.keras.layers.Input(shape=(64, 64, 12))

    # Encoder
    first = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(inputs)
    first = tf.keras.layers.LeakyReLU()(first)

    conv1 = tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(first)
    conv1 = tf.keras.layers.LeakyReLU()(conv1)
    conv1 = tf.keras.layers.BatchNormalization()(conv1)
    print(conv1.shape) # 32 32 32

    conv2 = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(conv1)
    conv2 = tf.keras.layers.GaussianNoise(0.1)(conv2)
    conv2 = tf.keras.layers.LeakyReLU()(conv2)
    conv2 = tf.keras.layers.BatchNormalization()(conv2)

    conv2b = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(conv2)
    conv2b = tf.keras.layers.LeakyReLU()(conv2b)
    conv2b = tf.keras.layers.BatchNormalization()(conv2b)+conv2
    print(conv2.shape) # 16 16 64

    conv3 = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(conv2b)
    conv3 = tf.keras.layers.LeakyReLU()(conv3)
    conv3 = tf.keras.layers.BatchNormalization()(conv3)
    print(conv3.shape) # 8 8 128

    # Decoder
    up1 = tf.keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(conv3)
    up1 = tf.keras.layers.LeakyReLU()(up1)
    up1 = tf.keras.layers.BatchNormalization()(up1)
    merge1 = tf.keras.layers.concatenate([up1, conv2], axis=3)
    print(merge1.shape) # 16 16 64

    up2 = tf.keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(merge1)
    up2 = tf.keras.layers.LeakyReLU()(up2)
    up2 = tf.keras.layers.BatchNormalization()(up2)
    merge2 = tf.keras.layers.concatenate([up2, conv1], axis=3)
    print(conv2.shape) # 32 32 32

    up3 = tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(merge2)
    up3 = tf.keras.layers.LeakyReLU()(up3)
    up3 = tf.keras.layers.BatchNormalization()(up3)
    merge3 = tf.keras.layers.concatenate([up3, first], axis=3)
    print(merge3.shape) # 64 64 16

    # Output layer
    outputs = tf.keras.layers.Conv2D(3, (3, 3), padding='same', activation='relu')(merge3)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)


    return model


# Replace 'folder_path' with the path to your folder containing images
folder_path = 'C:/Users/G_Cha/Videos/ShortWater'
images = load_images(folder_path)


generator = build_generator()
discriminator = tf.keras.models.load_model('./checkpoints/discriminator1.keras')






cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)



generator_optimizer = tf.keras.optimizers.Adam(1e-4)



BATCH_SIZE = 16

#@tf.function
def train_step(input_images, best_output):

    with tf.GradientTape() as gen_tape:

        #print(input_images.shape) #(16, 64, 64, 12)
        generated_images = generator(input_images, training=True)
        #print(generated_images.shape) #(16, 64, 64, 3)

        combined = tf.concat([input_images, generated_images], axis=-1)
        #print(combined.shape)

        fake_output = discriminator(combined, training=True)
        gen_loss = generator_loss(fake_output)+tf.reduce_mean(tf.square(generated_images - best_output))

        tf.print("Generator Loss:", gen_loss)


    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))


while True:

    for i in range(2000):

        batch_x, batch_y = create_batch(images, 16)

        train_step(batch_x, batch_y)


    generator.save('./checkpoints/generator.keras')

    def add_stack(stack, image):

        print(stack.shape) #(1, 64, 64, 12)
        print(image.shape) #(1, 64, 64, 3)


        stacked = np.concatenate([stack, image], axis=-1)
        print(stacked.shape)

        stacked_stripped = stacked[:, :, :, 3:]
        print(stacked_stripped.shape)


        return stacked_stripped


    batch_x, batch_y = create_batch(images, 16)

    starting_point = batch_x[0]
    starting_point = np.expand_dims(starting_point, axis=0)
    print(f"starting point shape: {starting_point.shape}")


    for i in range(120):

        predicted_image = generator.predict(starting_point)
        print(predicted_image.shape)
        predicted_image = np.clip(predicted_image, 0, 1)
        print(predicted_image.shape)
        save_path = os.path.join("C:/Users/G_Cha/Videos/Predicted", f"predicted_image_{i}.png")
        n = predicted_image[0]
        plt.imsave(save_path, n)  # Save the image using matplotlib

        starting_point = add_stack(starting_point, predicted_image)
        print(f"starting point shape: {starting_point.shape}")
